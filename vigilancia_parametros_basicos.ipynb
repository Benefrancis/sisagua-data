{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade distributed\n",
    "%pip install mariadb SQLAlchemy\n",
    "%pip install --upgrade dask"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import required modules\n",
    "import urllib \n",
    "from urllib import parse\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, inspect, text\n",
    "import dask.dataframe as dd\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46ba4e356384742c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def sql_df(q):\n",
    "  with engine.connect() as conexao:\n",
    "    consulta = conexao.execute(text(q))\n",
    "    dados = consulta.fetchall()\n",
    "  return pd.DataFrame(dados,columns=consulta.keys())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91a5a5e1d145871c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Codificar a senha\n",
    "senha_codificada = urllib.parse.quote(\"root\")\n",
    "\n",
    "# Criar a URL de conexão\n",
    "url_conexao = f\"mariadb+mariadbconnector://root:{senha_codificada}@127.0.0.1:3306/python-sisagua\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d1308b6f5395f1e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Criar o objeto da engine\n",
    "engine = create_engine(url_conexao, pool_recycle=3600)  # 1 hora de tempo de reciclagem"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1c5239f1693d519",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "inspector = inspect(engine)\n",
    "print(inspector.get_table_names())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac6fffa17e8baf2b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformando os dados em arquivo parquet\n",
    "\n",
    "Ao ler o arquivo Parquet, o Dask automaticamente cria um DataFrame Dask particionado para suportar operações paralelas. O número de partições dependerá de como o arquivo Parquet foi particionado originalmente. O Dask tenta ler cada parte em uma partição separada sempre que possível.\n",
    "\n",
    "Depois de ler o arquivo Parquet para um DataFrame Dask, você pode realizar operações distribuídas em seus dados de forma semelhante a como faria com um DataFrame pandas.\n",
    "\n",
    "Gravando e depois lendo os dados do arquivo .parquet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4756554be743f99"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Atenção são mais de 2.000.000 de registros em cada tabela\n",
    "for tabela in inspector.get_table_names(): \n",
    "    \n",
    "    query = f'SELECT * FROM {tabela}'\n",
    "    print(query)\n",
    "    \n",
    "    df = sql_df(query)\n",
    "    # Criar um DataFrame Dask a partir do DataFrame Pandas\n",
    "    df_dask = dd.from_pandas(df, npartitions=16)  # Você pode ajustar o número de partições conforme necessário\n",
    "    \n",
    "    caminho_arquivo = f'data/{tabela}/{tabela}'\n",
    "    \n",
    "    arquivo = f'{caminho_arquivo}.parquet' \n",
    "    \n",
    "    if not os.path.exists(caminho_arquivo): \n",
    "        os.makedirs(caminho_arquivo)\n",
    "    \n",
    "    # Salvar o DataFrame Dask no formato Parquet\n",
    "    df_dask.to_parquet(\n",
    "        arquivo,\n",
    "        # write_options={'compression': 'gzip'},\n",
    "        engine='pyarrow',  # ou 'fastparquet', dependendo da sua preferência\n",
    "        overwrite=True\n",
    "    )\n",
    "\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5f9efe47efdeacc",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fef7d6c3e092881"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for tabela in inspector.get_table_names(): \n",
    "    caminho_arquivo = f'data/{tabela}/{tabela}'\n",
    "    \n",
    "    arquivo = f'{caminho_arquivo}.parquet'\n",
    "    \n",
    "    # Ler o arquivo Parquet para um DataFrame Dask\n",
    "    df_dask_lido = dd.read_parquet(arquivo)\n",
    "    # Visualizar as primeiras linhas do DataFrame Dask lido\n",
    "    df_dask_lido.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76a4b1579f9c90e9",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
